{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append('..')\n",
    "from rul_common.config import ConfigTrain as ct\n",
    "from rul_common.utils_train import sensors\n",
    "from rul_common.utils_train import header\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "import warnings\n",
    "\n",
    "#modeling part\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "\n",
    "from scipy.stats import randint\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score, classification_report, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import PrecisionRecallDisplay\n",
    "\n",
    "import shap\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.float_format',  '{:,}'.format)\n",
    "\n",
    "\n",
    "\n",
    "def read_in(file1, file2, file3, file4):\n",
    "    df1 = pd.read_csv(file1, sep = \" \", header = 0, names = header)\n",
    "    df1['dataset'] = \"FD001\"\n",
    "    df2 = pd.read_csv(file2, sep = \" \", header = 0, names = header)\n",
    "    df2['dataset'] = \"FD002\"\n",
    "    df3 = pd.read_csv(file3, sep = \" \", header = 0, names = header)\n",
    "    df3['dataset'] = \"FD003\"\n",
    "    df4 = pd.read_csv(file4, sep = \" \", header = 0, names = header)\n",
    "    df4['dataset'] = \"FD004\"\n",
    "    return df1, df2, df3, df4\n",
    "\n",
    "def concatenation_x2(df1, df2):\n",
    "    train = pd.concat([df1, df2], ignore_index=True)\n",
    "    train = train.drop([\"blank1\", \"blank2\"], axis = 1)\n",
    "    return train\n",
    "\n",
    "\n",
    "def remining_cycles(train):\n",
    "    maxi_df = pd.DataFrame(train.groupby([\"esn\", \"dataset\"])[\"cycles\"].max().reset_index())\n",
    "    maxi_df.rename(columns = {\"cycles\" : \"max_cycles\"}, inplace = True)\n",
    "    train = train.merge(maxi_df, how = \"left\", on = [\"esn\", \"dataset\"])\n",
    "    train['rul'] = train['cycles'] - train['max_cycles']\n",
    "    return train\n",
    "\n",
    "n_clusters = 6\n",
    "def clustering(train):\n",
    "    kmeans = KMeans(n_clusters = n_clusters, \n",
    "                    random_state = 0, \n",
    "                    n_init=\"auto\").fit(train[[\"opset1\",\"opset2\"]].to_numpy())\n",
    "    train[\"condition\"] = kmeans.labels_\n",
    "    return train\n",
    "\n",
    "# lead time is a parameter to \n",
    "lead_time = -100\n",
    "def make_y(row):\n",
    "    \"\"\"lead time is a parameter to be decided with what anticipation the business needs to know \n",
    "    about a failure to have enough time for action.\n",
    "    100 cycles is a convenient anticipation\n",
    "\n",
    "    This function creates Y_target value if ESN is less that 100 cycles to the Failure or more\n",
    "    \"\"\"\n",
    "    return 1 if row[\"rul\"] > lead_time else 0\n",
    "\n",
    "def one_hot(df, column, names=None):\n",
    "    enc = OneHotEncoder()\n",
    "    labelenc = LabelEncoder()\n",
    "    if names == None:\n",
    "        names = df[column].unique().tolist()\n",
    "    df['column_cat'] = labelenc.fit_transform(df[column])\n",
    "    enc_df = pd.DataFrame(enc.fit_transform(df[column].to_numpy().reshape(-1,1)).toarray(), columns = names)\n",
    "    enc_df.drop(columns = names[0], inplace = True)\n",
    "    df = pd.concat([df, enc_df], axis = 1)\n",
    "    df.drop(columns = column, inplace = True)\n",
    "    df.drop(columns = 'column_cat', inplace = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFD001, dfFD002, dfFD003, dfFD004 = read_in(ct.location1, ct.location2, ct.location3, ct.location4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainHPC = concatenation_x2(dfFD001, dfFD003)\n",
    "trainHPC_Fan = concatenation_x2(dfFD002, dfFD004)\n",
    "trainHPC = remining_cycles(trainHPC)\n",
    "trainHPC_Fan = remining_cycles(trainHPC_Fan)\n",
    "trainHPC = clustering(trainHPC)\n",
    "trainHPC_Fan = clustering(trainHPC_Fan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Rozdzielnie dwa datasety analizować : a. samo HPC, b. HPC plus Fan\n",
    "2. model na anomaly detection dla danego Failure Mode (razy dwa bo dwa datasety)\n",
    "3. wiem, że ostatnie 10 cykli to na pewno chorzy a pierwsze 10 w treningowej to na pewno zdrowi\n",
    "4. wielka dziura w środku jak zaklasyfikować obserw od 10 do -10\n",
    "5. z obrazkow widać, że ostatnie N cykli dla każdego silnika to widać zuzycie - chcemy znaleźć N\n",
    "6. zeby znalexc N spr dla jakiego N dostaniemy najlepszy model klasyfikacyjny.\n",
    "7. przy jakim N jakość klasyfikacji znacznie spadnie.\n",
    "8. clustering na anomally detection niemożliwy bo nie są to rozdzielne chmury punktów - ciągle \n",
    "9. czemu nie wyznaczyć anomalli na na y na wartość sensora? Bo mamy kilka warunków lotu więc brak jednego uniwersalnego y\n",
    "10. jeśli warunki na datasety i oper params to rozbimi złożony algo regułowy. \n",
    "11. dobieranie tej wartosci recznie to zmudne - trenowanie modeli. Light sprawdza czułość na każdym rozgałezieniu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "przyjmuje lead time z zakresu -150 do -10 zwraca AUC i AP\n",
    "\n",
    "Jak to zrobię to wybieram najlepszy leadtime = max AP lub AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mamy lead time i model clas czy silnik jest zdrowy czy chory.\n",
    "i tu gdzie chory to mam prawdopodob, że chory i biorę tę zmienną jako zmienną pomocniczą do liczenia RUL.\n",
    "Podejscie 1. Dostawiamy kolumnę pred_proba, że chory i liczymy RUL na wszystkim RNN\n",
    "Podejście 2. filtrujemy po prawdd, że chory, trenujemy RNN tylko na cyyklach chorych i tu, gdzie prawd bardzo małe dopisujemy ręcznie, że RUL większy niż lead time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aec0c61be849e781be29b4325fa7d3a7284444d258f86bfe2a8dc98591d4c09f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
